{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje automático y aplicaciones\n",
    "\n",
    "## Regresión (caso de estudio)\n",
    "\n",
    "---\n",
    "$A^3$ @ FI-UNER : 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre el conjunto de datos\n",
    "\n",
    "En la ciudad de Ames, Iowa, se realizó el registro de diversas transacciones de compra-venta inmobiliaria desde 2006 a 2010. **El objetivo es identificar variables que permitan predecir el precio de venta de la propiedad**, con el fin de facilitar estimaciones regulatorias y ofrecer una alternativa de tasación a los ciudadanos. Es decir se desea construir un regresor que tome los atributos (o una selección de ellos) y que estime el precio de venta de la propiedad:\n",
    "\n",
    "$$ \\text{ Atributos } \\rightarrow \\text{Precio de venta}$$\n",
    "\n",
    "**El dataset contiene 2919 observaciones y un gran número de variables explicativas (23 nominales, 23 ordinales, 14 numéricas discretas, y 20 numéricas contínuas)**. El dataset esta disponible en [este link](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). En ese link encontrará un archivo `data_description.txt` que explica todas las variables y sus posibles valores. También encontrará dos archivos de datos: `train.csv` y `test.csv` (cada unoarchivo con aproximadamente 1460 observaciones).\n",
    " \n",
    "El objetivo es hacer el análisis exploratorio y entrenamiento de los modelos con `train.csv`, y luego usar `test.csv` para evaluar el modelo final en datos nunca vistos. El archivo `test.csv` no tiene las etiquetas (el precio de las casas) sino que se utilizará el sistema de Kaggle para enviar las predicciones y así obtener la medida de error. \n",
    "Este tipo de esquema asegura que las etiquetas de la particion de test NO influencien el desarrollo, y así evitan el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de las librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas-profiling\\[notebook\\] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los datos y exploración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.columns, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Las herramientas de análisis más potentes, como `pandas-profiling`, nos da información variada y algunas pistas sobre como seguir con el preprocesamiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ProfileReport(df_train, title=\"Analisis exploratorio inicial\", minimal=True)  # Reporte minimo\n",
    "# ProfileReport(df_train.sample(frac=.10), title=\"Analisis exploratorio inicial\")  # Reporte con una porcion de los datos\n",
    "# ProfileReport(df_train, title=\"Analisis exploratorio inicial\", interactions={\"targets\": [\"SalePrice\"]})\n",
    "\n",
    "ProfileReport(df_train, title=\"Analisis exploratorio inicial\", interactions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Descripción de los datos\n",
    "\n",
    "Si contamos con ella también es útil al momento del análisis exploratorio. Ver `data_description.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Por ejemplo, podemos descubrir que:\n",
    "- Unas cuantas variables usan el valor \"NA\" como dato válido, y pandas por defecto lo toma como `nan`.\n",
    "- Hay variables categóricas que se toman como numéricas (por ejemplo MSSubClass)\n",
    "- Hay variables categóricas ordinales que no se toman como tales (por ejemplo KitchenQual o GarageQual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train.Alley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recarga de los datos con conversión de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para convertir valores\n",
    "def NA2None(cell):\n",
    "    if cell == \"NA\":\n",
    "        return \"None\"\n",
    "    \n",
    "NA_converter = {\"Alley\": NA2None}\n",
    "\n",
    "df_train = pd.read_csv('train.csv', converters=NA_converter) \n",
    "df_test = pd.read_csv('test.csv', converters=NA_converter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.Alley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(df_train, title=\"Analisis exploratorio inicial\", interactions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacción con algunas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrLivArea: superficie habitable sin contar el sotano\n",
    "df_train.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n",
    "# OverallQual: calidad general de la vivienda\n",
    "df_train.plot.scatter(x=\"OverallQual\", y=\"SalePrice\")\n",
    "# LandSlope: inclinación del terreno \n",
    "df_train.plot.scatter(x=\"LandSlope\", y=\"SalePrice\");\n",
    "# Neighborhood: barrio\n",
    "df_train.plot.scatter(x=\"Neighborhood\", y=\"SalePrice\", rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los datos\n",
    "\n",
    "Si bien queremos construir un modelo basado en los datos, no siempre es bueno utilizar todos los datos tal cual están. Por ejemplo, podríamos tomar criterios como los siguientes para comenzar el modelado. Consideraciones para filtrar:\n",
    "- registros de menor superficie habitable \n",
    "- operaciones anotadas como normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (df_train.GrLivArea <= 2500) & (df_train.SaleCondition == \"Normal\")\n",
    "\n",
    "df_train_rev = df_train.loc[criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrLivArea: superficie habitable sin contar el sotano\n",
    "df_train_rev.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n",
    "# OverallQual: calidad general de la vivienda\n",
    "df_train_rev.plot.scatter(x=\"OverallQual\", y=\"SalePrice\")\n",
    "# LandSlope: inclinación del terreno \n",
    "df_train_rev.plot.scatter(x=\"LandSlope\", y=\"SalePrice\");\n",
    "# Neighborhood: barrio\n",
    "df_train_rev.plot.scatter(x=\"Neighborhood\", y=\"SalePrice\", rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_train_rev.loc[:, 'SalePrice'], kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se acerca a una distribución gaussiana?\n",
    "stats.probplot(df_train_rev.loc[:, 'SalePrice'], plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de variables de entrada y/o salida\n",
    "\n",
    "Si lo consideramos necesario se pueden hacer transformación de variables. Por ejemplo, se podría aplicar alguna función a la salida esperada (con el cuidado de invertir la transformación al momento de predecir los valores finales). Más adelante veremos otra estrategia para incorporarlo al pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rev.loc[:, \"SalePriceLog\"] = np.log1p(df_train_rev.loc[:, 'SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_train_rev[\"SalePriceLog\"], kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se acerca a una distribución gaussiana?\n",
    "res = stats.probplot(df_train_rev[\"SalePriceLog\"], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "También se podrían transformar las variables de entrada. Tener en cuenta que al hacerlo \"manualmente\" deberíamos aplicar lo mismo sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de la matriz de entrada y el vector de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = [\"GrLivArea\", \"OverallQual\"]\n",
    "\n",
    "X_train = df_train_rev.loc[:, features_col].values\n",
    "y_train = df_train_rev.loc[:, \"SalePrice\"].values\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resultado con Regresión Lineal\n",
    "\n",
    "[sklearn LinearRegression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regLR = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores_to_use = (\"r2\", \"neg_mean_squared_error\")\n",
    "\n",
    "# Validación cruzada con folds y scores definidos\n",
    "cv_scores = cross_validate(regLR, X_train, y_train, cv=5, scoring=scores_to_use)\n",
    "results[\"LR\"] = cv_scores\n",
    "\n",
    "for sc in scores_to_use:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]),\n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Solo realizado a los fines de graficación, no sería necesario.\n",
    "cv_predicts = cross_val_predict(regLR, X_train, y_train, cv=5)\n",
    "        \n",
    "plt.scatter(cv_predicts, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si lo entrenáramos con todos los datos tendríamos una gráfica \"equivalente\" \n",
    "# pero sin poder analizar resultados (porque train y validation son lo mismo)\n",
    "regLR.fit(X_train, y_train)\n",
    "y_pred = regLR.predict(X_train)\n",
    "\n",
    "plt.scatter(y_pred, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de la salida (cliping)\n",
    "\n",
    "[sklearn TransformedTargetRegressor](https://scikit-learn.org/stable/modules/compose.html#transforming-target-in-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Regresor lineal básico con transformación de la salida\n",
    "regLRclip = TransformedTargetRegressor(\n",
    "    regressor=regLR, \n",
    "    func=lambda x: x,                                   # lineal\n",
    "    inverse_func=lambda x: np.clip(x, 50_000, 350_000), # establece limites\n",
    "    check_inverse=False\n",
    ")\n",
    "\n",
    "cv_scores = cross_validate(regLRclip, X_train, y_train, cv=5, scoring=scores_to_use)  \n",
    "results[\"LRclip\"] = cv_scores\n",
    "\n",
    "for sc in scores_to_use:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]),\n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predicts = cross_val_predict(regLRclip, X_train, y_train, cv=5)\n",
    "plt.scatter(cv_predicts, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de la salida (log exp)\n",
    "\n",
    "[sklearn TransformedTargetRegressor](https://scikit-learn.org/stable/modules/compose.html#transforming-target-in-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Regresor lineal básico con transformación de la salida\n",
    "regLRlog = TransformedTargetRegressor(\n",
    "    regressor=regLR, \n",
    "    func=np.log1p,         # log(1 + x)\n",
    "    inverse_func=np.expm1  # exp(x) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(regLRlog, X_train, y_train, cv=5, scoring=scores_to_use)\n",
    "results[\"LRlog\"] = cv_scores\n",
    "\n",
    "for sc in scores_to_use:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]), \n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predicts = cross_val_predict(regLRlog, X_train, y_train, cv=5)\n",
    "plt.scatter(cv_predicts, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de las variables de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = [\"GrLivArea\", \"OverallQual\", \"LandSlope\", \"Neighborhood\"]\n",
    "\n",
    "X_train = df_train_rev.loc[:, features_col].values\n",
    "X_test = df_test.loc[:, features_col].values\n",
    "y_train = df_train_rev.loc[:, \"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "# Se definen las transformaciones para las entradas\n",
    "preprocessor = make_column_transformer(\n",
    "#     (\"passthrough\", [0, 1, 2, 3]),\n",
    "    (StandardScaler(), [0, 1]),\n",
    "#     (MinMaxScaler(), [0, 1]),\n",
    "    (OrdinalEncoder(categories=[(\"Gtl\", \"Mod\", \"Sev\")]), [2]),\n",
    "    (OneHotEncoder(), [3]),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Display resultados solo a los fines de ejemplificar\n",
    "display(\"original\", X_train[212:215])\n",
    "display(\"transformed\", preprocessor.fit_transform(X_train[212:215]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apilado del preprocesamiento y el regresor con transformación de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "preLRlog = make_pipeline(\n",
    "    preprocessor,\n",
    "    regLRlog\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "preLRlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(preLRlog, X_train, y_train, cv=5, scoring=scores_to_use)\n",
    "results[\"preLRlog\"] = cv_scores\n",
    "\n",
    "for sc in scores_to_use:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]), \n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predicts = cross_val_predict(preLRlog, X_train, y_train, cv=5)\n",
    "plt.scatter(cv_predicts, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optar por otro tipo de regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "preRFlog = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestRegressor(n_estimators=50, max_depth=4, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(preRFlog, X_train, y_train, cv=5, scoring=scores_to_use[1:])\n",
    "results[\"preRFlog\"] = cv_scores\n",
    "\n",
    "# Ya no podemos usar R^2 como métrica, el modelo no es lineal\n",
    "for sc in scores_to_use[1:]:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]), \n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predicts = cross_val_predict(preRFlog, X_train, y_train, cv=5)\n",
    "plt.scatter(cv_predicts, y_train)\n",
    "plt.xlabel(\"Valores Estimados\")\n",
    "plt.ylabel(\"Valores Verdaderos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizar un hiperparámetro del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preRFlog.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_to_explore = {\"n_estimators\": [5, 50, 500]}\n",
    "\n",
    "# En el pipeline mismo se define una busquerda de los hiperparámetros\n",
    "preRFcvlog = make_pipeline(\n",
    "    preprocessor,\n",
    "    GridSearchCV(RandomForestRegressor(max_depth=4, random_state=42), \n",
    "                 param_grid=param_to_explore, verbose=2, cv=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(preRFcvlog, X_train, y_train, cv=5, scoring=scores_to_use[1:])\n",
    "results[\"preRFcvlog\"] = cv_scores\n",
    "\n",
    "# Ya no podemos usar R^2 como métrica, el modelo no es lineal\n",
    "for sc in scores_to_use[1:]:\n",
    "    print(sc, \n",
    "          cv_scores[f\"test_{sc}\"],\n",
    "          np.mean(cv_scores[f\"test_{sc}\"]), \n",
    "          \"\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"test_neg_mean_squared_error\"\n",
    "results_to_plot = {}\n",
    "\n",
    "for reg_name in results:\n",
    "    results_to_plot[reg_name] = results[reg_name][metric_to_plot] * -1.0\n",
    "\n",
    "results_to_plot = pd.DataFrame(results_to_plot)\n",
    "results_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_to_plot.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"variable\", y=\"value\" ,data=results_to_plot.melt());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones sobre el conjunto de test\n",
    "\n",
    "Considerando los resultados obtenidos, seleccionamos el mejor modelo, lo entrenamos con la partición de train y predecimos sobre la partición de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preLRlog.fit(X_train, y_train)  # Entrenamiento con todos los datos de train\n",
    "\n",
    "y_pred = preLRlog.predict(X_test)  # Predicciones sobre todos los de test\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar archivo para subir a kaggle\n",
    "\n",
    "submission_df = pd.DataFrame({\"Id\": df_test.loc[:, \"Id\"], \n",
    "                              \"SalePrice\": y_pred})\n",
    "\n",
    "submission_df.to_csv(\"submission_preLRlog.csv\", index=False)\n",
    "\n",
    "# Resultado obtenido: \n",
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "# 0.17366 Root mean squared logarithmic error\n",
    "\n",
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
